{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分类（Sentiment Analysis）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验前导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-04T08:53:29.797788100Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "# from torchtext.data import Field\n",
    "import random\n",
    "SEED = 1024\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）首先准备训练数据、验证数据和测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用torchtext自带的IMDB数据集进行情感分析：检测一段文字的情感是正面的还是负面的\n",
    "- TorchText中的一个重要概念是Field。Field决定了你的数据会被怎样处理。在情感分类任务中，我们所要接触到的数据又文本字符串和两种情感，'pos'和'neg'；\n",
    "- 使用TEXT Field来定义如何处理电影评论，使用Label Field来处理两个情感类别；\n",
    "- torchtext能够处理很多自然语言中处理到的任务（创建vocabulary，然后把数据读成Batch的格式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=\"spacy\",tokenizer_language='en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己添加代码离线存储数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 假设 train_data 是一个 torchtext.datasets.imdb.IMDB 对象\n",
    "train_data_list = [(example.text, example.label) for example in train_data]\n",
    "# 保存数据\n",
    "with open('train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#保存test\n",
    "test_data_list = [(example.text, example.label) for example in test_data]\n",
    "# 保存数据\n",
    "with open('test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Zentropa', 'has', 'much', 'in', 'common', 'with', 'The', 'Third', 'Man', ',', 'another', 'noir', '-', 'like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'Europe', '.', 'Like', 'TTM', ',', 'there', 'is', 'much', 'inventive', 'camera', 'work', '.', 'There', 'is', 'an', 'innocent', 'American', 'who', 'gets', 'emotionally', 'involved', 'with', 'a', 'woman', 'he', 'does', \"n't\", 'really', 'understand', ',', 'and', 'whose', 'naivety', 'is', 'all', 'the', 'more', 'striking', 'in', 'contrast', 'with', 'the', 'natives.<br', '/><br', '/>But', 'I', \"'d\", 'have', 'to', 'say', 'that', 'The', 'Third', 'Man', 'has', 'a', 'more', 'well', '-', 'crafted', 'storyline', '.', 'Zentropa', 'is', 'a', 'bit', 'disjointed', 'in', 'this', 'respect', '.', 'Perhaps', 'this', 'is', 'intentional', ':', 'it', 'is', 'presented', 'as', 'a', 'dream', '/', 'nightmare', ',', 'and', 'making', 'it', 'too', 'coherent', 'would', 'spoil', 'the', 'effect', '.', '<', 'br', '/><br', '/>This', 'movie', 'is', 'unrelentingly', 'grim--\"noir', '\"', 'in', 'more', 'than', 'one', 'sense', ';', 'one', 'never', 'sees', 'the', 'sun', 'shine', '.', 'Grim', ',', 'but', 'intriguing', ',', 'and', 'frightening', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    train_data_list = pickle.load(f)\n",
    "print(train_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------分割线，自己的代码结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "{'text': ['Zentropa', 'has', 'much', 'in', 'common', 'with', 'The', 'Third', 'Man', ',', 'another', 'noir', '-', 'like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'Europe', '.', 'Like', 'TTM', ',', 'there', 'is', 'much', 'inventive', 'camera', 'work', '.', 'There', 'is', 'an', 'innocent', 'American', 'who', 'gets', 'emotionally', 'involved', 'with', 'a', 'woman', 'he', 'does', \"n't\", 'really', 'understand', ',', 'and', 'whose', 'naivety', 'is', 'all', 'the', 'more', 'striking', 'in', 'contrast', 'with', 'the', 'natives.<br', '/><br', '/>But', 'I', \"'d\", 'have', 'to', 'say', 'that', 'The', 'Third', 'Man', 'has', 'a', 'more', 'well', '-', 'crafted', 'storyline', '.', 'Zentropa', 'is', 'a', 'bit', 'disjointed', 'in', 'this', 'respect', '.', 'Perhaps', 'this', 'is', 'intentional', ':', 'it', 'is', 'presented', 'as', 'a', 'dream', '/', 'nightmare', ',', 'and', 'making', 'it', 'too', 'coherent', 'would', 'spoil', 'the', 'effect', '.', '<', 'br', '/><br', '/>This', 'movie', 'is', 'unrelentingly', 'grim--\"noir', '\"', 'in', 'more', 'than', 'one', 'sense', ';', 'one', 'never', 'sees', 'the', 'sun', 'shine', '.', 'Grim', ',', 'but', 'intriguing', ',', 'and', 'frightening', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(vars(train_data.examples[0]))#每个example包含两个部分，第一部分为text,第二部分为label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.example.Example"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据类型\n",
    "type(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 由于我们现在只有train/test两个分类，所以需要创建一个新的validation set，可以使用.split()创建新的分类。\n",
    "- 默认的数据分割是70、30，如果我们声明split_ratio，可以改变split之间的比例，split_ratio=0.8表示80%的数据是训练集，20%是验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED)) #默认使用70%作为训练，30%作为验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "# 至此已经完成了训练数据，测试数据和验证数据的分割\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）其次进行数据预处理：将单词和标签转换为index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 从训练数据中构建词典\n",
    "vectors = Vectors(name = './glove.6B.100d.txt')\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors=vectors,unk_init=torch.Tensor.normal_)\n",
    "#使用glove预训练的词向量,词向量在：https://nlp.stanford.edu/projects/glove/中下载\n",
    "#TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABEL.build_vocab(train_data)\n",
    "# build_vocab的本质是把一个单词map到一个index上面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}') # 25000个高频单词+<unk>+<pad>\n",
    "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202342), (',', 192592), ('.', 165448), ('a', 109503), ('and', 109493), ('of', 100784), ('to', 93530), ('is', 76110), ('in', 61318), ('I', 54054)]\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n",
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(10))\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （3）创建iterator，将数据按照batch返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用BucketIterator,BucketIterator会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:03.212445600Z",
     "start_time": "2024-01-04T07:30:03.114452800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits( \n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.383206500Z",
     "start_time": "2024-01-04T07:30:03.313446700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "# batch.label #代表标签\n",
    "# batch.text #每一列是一个句子，以1结尾，1代表pad\n",
    "# batch.text[:,0] #第一列\n",
    "#print(' '.join(TEXT.vocab.itos[i] for i in batch.text[:,0]))\n",
    "#[TEXT.vocab.itos[i] for i in batch.text[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.552201600Z",
     "start_time": "2024-01-04T07:30:21.384203Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 'm a big fan of <unk> , having caught on only at the end of season three . I faithfully watched all the previous seasons when it was <unk> , concluding that it was one of the most well - thought out story arcs to ever hit television . Even the filler episodes were interesting . The movies , also , were well produced and as entertaining as anything to hit the <unk> /><br />Which brings us to ' River of Souls ' . Naturally , after seeing everything else , I had high expectations . Martin Sheen appears to be acting in an Ed Wood movie rather than a serious Sci - Fi story . The story itself , might have looked good in outline form , even made it to the story board . However , it suffers obviously when it came time to filling this notion out into a two hour movie . There are no special effects to keep us entertained in the total absence of a compelling story . There are places where they were obviously short of time and just improvised the dialog to fill the story out . Had this made the regular season , it would have rated among the worst of the episodes . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(TEXT.vocab.itos[i] for i in batch.text[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照模型从简单到复杂，依次构建三种模型：\n",
    "\n",
    "- Word Averaging模型\n",
    "- RNN/LSTM模型\n",
    "- CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）Word Averaging模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 把每个单词都通过Embedding层映射成word embedding vector,然后把所有的word vector做个平均，就是整个句子的vector表示了，接下来把这个sentence vector传入一个Linear层，做分类即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.563200400Z",
     "start_time": "2024-01-04T07:30:21.522202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,output_size,pad_idx):\n",
    "        super(WordAVGModel,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,padding_idx=pad_idx)#padding_idx会将<pad>初始化为全0\n",
    "        self.linear = nn.Linear(embedding_size,output_size)\n",
    "    def forward(self,text):\n",
    "        embeded = self.embed(text) # [sequence_len, batch_size, embedding_size]\n",
    "        #embeded = embeded.transpose(1,0) #[batch_size, sequence_len, embedding_size]\n",
    "        embeded = embeded.permute(1,0,2) #[batch_size, sequence_len, embedding_size]\n",
    "        pooled = F.avg_pool2d(embeded,(embeded.shape[1],1)).squeeze() # embeded.shape[1] = sequence_len\n",
    "        return self.linear(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.806214300Z",
     "start_time": "2024-01-04T07:30:21.593203100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "wordmodel = WordAVGModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.829206600Z",
     "start_time": "2024-01-04T07:30:21.808200700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.898200500Z",
     "start_time": "2024-01-04T07:30:21.827201700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500301"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    # next(model.parameters()).numel()\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(wordmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.921203900Z",
     "start_time": "2024-01-04T07:30:21.842206200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化Embedding矩阵\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "#pretrained_embedding.shape # [25002,100]\n",
    "wordmodel.embed.weight.data.copy_(pretrained_embedding) #用事先训练好的embeeding来初始化\n",
    "wordmodel.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "wordmodel.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:30:21.921203900Z",
     "start_time": "2024-01-04T07:30:21.875202100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds,y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()#(rounded_preds == y)返回的是bool类型的结果\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:31:30.394576400Z",
     "start_time": "2024-01-04T07:31:30.355569500Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    total_len = 0.\n",
    "    model.train()    \n",
    "    for batch in iterator:\n",
    "        # forward pass\n",
    "        # print(\"batch.text.shape:\",batch.text.shape)\n",
    "        preds = model(batch.text).squeeze()\n",
    "        # print(\"preds.shape:\",preds.shape)\n",
    "        # print(\"batch.label.shape:\",batch.label.shape)\n",
    "        loss = crit(preds,batch.label)\n",
    "        acc = binary_accuracy(preds,batch.label)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() #梯度清零\n",
    "        loss.backward() # 反向传播，计算梯度\n",
    "        optimizer.step() #更新参数\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    return epoch_loss/total_len, epoch_acc/total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:31:30.891565200Z",
     "start_time": "2024-01-04T07:31:30.838564Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    total_len = 0.\n",
    "    model.eval()    \n",
    "    for batch in iterator:\n",
    "        # forward pass\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = crit(preds,batch.label)\n",
    "        acc = binary_accuracy(preds,batch.label)\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    model.train() \n",
    "    return epoch_loss/total_len, epoch_acc/total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:54:32.916866800Z",
     "start_time": "2024-01-04T02:52:29.612637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.6863600972993034 Train Acc: 0.5822857142993382\n",
      "Epoch: 0 Valid Loss: 0.6228502765019734 Valid Acc: 0.7302666666666666\n",
      "Epoch: 1 Train Loss: 0.6456488602093288 Train Acc: 0.7197714286804199\n",
      "Epoch: 1 Valid Loss: 0.4932324107170105 Valid Acc: 0.7682666667302449\n",
      "Epoch: 2 Train Loss: 0.5722647533144269 Train Acc: 0.7908000000817436\n",
      "Epoch: 2 Valid Loss: 0.42996904436747235 Valid Acc: 0.8080000000635783\n",
      "Epoch: 3 Train Loss: 0.49981179449898855 Train Acc: 0.8297142857824054\n",
      "Epoch: 3 Valid Loss: 0.3975183746337891 Valid Acc: 0.835066666730245\n",
      "Epoch: 4 Train Loss: 0.4350914260183062 Train Acc: 0.8602857143947056\n",
      "Epoch: 4 Valid Loss: 0.3858402350902557 Valid Acc: 0.8508000000635783\n",
      "Epoch: 5 Train Loss: 0.3872138711180006 Train Acc: 0.8795428572246007\n",
      "Epoch: 5 Valid Loss: 0.39407181444168093 Valid Acc: 0.8613333333969116\n",
      "Epoch: 6 Train Loss: 0.3474335999011993 Train Acc: 0.8909714286804199\n",
      "Epoch: 6 Valid Loss: 0.4040147019783656 Valid Acc: 0.8708000000317891\n",
      "Epoch: 7 Train Loss: 0.31704363434995925 Train Acc: 0.9010285714830671\n",
      "Epoch: 7 Valid Loss: 0.4208853840907415 Valid Acc: 0.8769333333651225\n",
      "Epoch: 8 Train Loss: 0.29154966223921097 Train Acc: 0.908742857170105\n",
      "Epoch: 8 Valid Loss: 0.4345247079292933 Valid Acc: 0.8808000000317892\n",
      "Epoch: 9 Train Loss: 0.2699590578556061 Train Acc: 0.9158857143674578\n",
      "Epoch: 9 Valid Loss: 0.4519823522925377 Valid Acc: 0.8848000000317892\n",
      "Epoch: 10 Train Loss: 0.2521127163001469 Train Acc: 0.9216571428843907\n",
      "Epoch: 10 Valid Loss: 0.46696190962791445 Valid Acc: 0.8873333333651224\n",
      "Epoch: 11 Train Loss: 0.2351403525182179 Train Acc: 0.9266285714830671\n",
      "Epoch: 11 Valid Loss: 0.4822789580066999 Valid Acc: 0.8900000000317891\n",
      "Epoch: 12 Train Loss: 0.22169148511546 Train Acc: 0.932114285823277\n",
      "Epoch: 12 Valid Loss: 0.4979425580541293 Valid Acc: 0.8918666666666667\n",
      "Epoch: 13 Train Loss: 0.20770146679197038 Train Acc: 0.9359428572246007\n",
      "Epoch: 13 Valid Loss: 0.5128956850369771 Valid Acc: 0.8925333333333333\n",
      "Epoch: 14 Train Loss: 0.19576083881514414 Train Acc: 0.9401142857687814\n",
      "Epoch: 14 Valid Loss: 0.5275528556863467 Valid Acc: 0.8937333333333334\n",
      "Epoch: 15 Train Loss: 0.18495418101038252 Train Acc: 0.9440571429388863\n",
      "Epoch: 15 Valid Loss: 0.5442172487298648 Valid Acc: 0.8929333333333334\n",
      "Epoch: 16 Train Loss: 0.1732836849783148 Train Acc: 0.9481714285714286\n",
      "Epoch: 16 Valid Loss: 0.55906378373305 Valid Acc: 0.8941333333333333\n",
      "Epoch: 17 Train Loss: 0.1635377680352756 Train Acc: 0.951542857170105\n",
      "Epoch: 17 Valid Loss: 0.5733730255683264 Valid Acc: 0.8952\n",
      "Epoch: 18 Train Loss: 0.15557307760715486 Train Acc: 0.9552571428843907\n",
      "Epoch: 18 Valid Loss: 0.5912954429864884 Valid Acc: 0.8942666666666667\n",
      "Epoch: 19 Train Loss: 0.14633685490744455 Train Acc: 0.9585142857415335\n",
      "Epoch: 19 Valid Loss: 0.6035241787830988 Valid Acc: 0.8972\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "crit = nn.BCEWithLogitsLoss()#binary cross entropy\n",
    "optimizer = torch.optim.Adam(wordmodel.parameters())\n",
    "wordmodel = wordmodel.to(device)\n",
    "N_EPOCHES = 20\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHES):\n",
    "    train_loss, train_acc = train(wordmodel,train_iterator,optimizer,crit)\n",
    "    valid_loss, valid_acc = evaluate(wordmodel,valid_iterator,crit)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(wordmodel.state_dict(),\"wordavg-best-model.pth\")\n",
    "    print(\"Epoch:\",epoch, \"Train Loss:\", train_loss, \"Train Acc:\", train_acc)\n",
    "    print(\"Epoch:\",epoch, \"Valid Loss:\", valid_loss, \"Valid Acc:\", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T02:54:32.962815900Z",
     "start_time": "2024-01-04T02:54:32.919818700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordmodel模型在测试集上表现为:\n",
      "测试损失Test Loss: 0.6876286530971527 测试精度Test Acc: 0.884640000038147\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "wordmodel.load_state_dict(torch.load(\"wordavg-best-model.pth\"))\n",
    "test_loss1, test_acc1 = evaluate(wordmodel,test_iterator,crit)\n",
    "print(\"wordmodel模型在测试集上表现为:\")\n",
    "print(\"测试损失Test Loss:\", test_loss1, \"测试精度Test Acc:\", test_acc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T03:04:10.650618800Z",
     "start_time": "2024-01-04T03:04:09.366744Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 利用模型来预测Sentiment\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device) # seq_len\n",
    "    tensor = tensor.unsqueeze(1) # seq_len * batch_size\n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()\n",
    "#函数作用\n",
    "#nlp.tokenizer(sentence)：将句子分词\n",
    "#[tok.text for tok in nlp.tokenizer(sentence)]：将分词后的结果转换为list\n",
    "#[TEXT.vocab.stoi[t] for t in tokenized]：将list中的每个单词转换为index\n",
    "#torch.LongTensor(indexed).to(device)：将index转换为tensor\n",
    "#tensor = tensor.unsqueeze(1)：将tensor的维度从[seq_len]转换为[seq_len,1]\n",
    "#pred = torch.sigmoid(model(tensor))：将tensor输入到模型中，得到预测结果\n",
    "#pred.item()：将tensor转换为python中的数字\n",
    "#上述函数的预测结果如何理解\n",
    "#pred.item()的值越大，表示句子的情感越正面；pred.item()的值越小，表示句子的情感越负面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T03:04:10.671618700Z",
     "start_time": "2024-01-04T03:04:10.654618200Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5682803492464694e-33"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is good\",wordmodel)#horrible,terrific,terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二个模型：RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "基于循环神经网络（RNN）的文本分类模型。下面是对该模型的分析：\n",
    "\n",
    "1、初始化函数（__init__）：\n",
    " - vocab_size：词汇表大小，表示文本数据中不同词汇的数量。\n",
    " - embedding_size：嵌入层的维度，用于将文本数据转换为稠密向量表示。\n",
    " - output_size：模型输出的维度，表示分类任务的类别数量。\n",
    " - pad_idx：填充标记的索引，在文本序列中用于标记填充部分。\n",
    " - hidden_size：RNN隐藏层的维度。\n",
    " - n_layers：RNN的层数。\n",
    " - bidirectional：是否使用双向RNN。\n",
    " - dropout：dropout比例，用于在训练过程中随机丢弃一部分神经元，以防止过拟合。\n",
    "\n",
    "\n",
    "2、前向传播函数（forward）：\n",
    " - embed：将输入的文本序列进行嵌入操作，将每个词汇映射为一个稠密向量。\n",
    " - rnn：将嵌入后的文本序列输入到LSTM中，得到输出序列、最后一个时间步的隐藏状态和细胞状态。\n",
    " - linear：将最后一个时间步的隐藏状态通过线性层进行分类预测，并返回预测结果。\n",
    " - dropout：对嵌入后的文本序列进行dropout操作。\n",
    "模型的输入输出维度：\n",
    "输入：文本序列的维度为[sequence_len, batch_size]，其中sequence_len表示序列的长度，batch_size表示每批次输入的样本数量。\n",
    "输出：模型的输出维度为[batch_size, output_size]，其中output_size表示分类任务的类别数量。\n",
    "该模型使用了LSTM作为RNN的基本单元，通过双向RNN和线性层将最后一个时间步的隐藏状态转化为分类预测结果。在前向传播过程中，使用了dropout操作来减少过拟合的风险。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T03:04:56.995347300Z",
     "start_time": "2024-01-04T03:04:56.980348400Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,output_size,pad_idx,hidden_size,n_layers,bidirectional,dropout):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers=n_layers,bidirectional=bidirectional,dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size*2,output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,text):\n",
    "        embeded = self.dropout(self.embed(text)) # [sequence_len, batch_size, embedding_size]\n",
    "        output, (hidden, cell) = self.rnn(embeded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))\n",
    "        return self.linear(hidden)\n",
    "#模型的输入输出维度\n",
    "#输入：[sequence_len, batch_size]\n",
    "#输出：[batch_size, output_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T03:06:30.944051Z",
     "start_time": "2024-01-04T03:06:30.872051900Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "HIDDEN_SIZE = 100\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "rnnmodel = RNNModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX,\n",
    "                     hidden_size=HIDDEN_SIZE,\n",
    "                     n_layers=N_LAYERS,\n",
    "                     bidirectional=BIDIRECTIONAL,\n",
    "                     dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T03:06:31.370561600Z",
     "start_time": "2024-01-04T03:06:31.337562400Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始化Embedding矩阵\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "#pretrained_embedding.shape # [25002,100]\n",
    "rnnmodel.embed.weight.data.copy_(pretrained_embedding) #用事先训练好的embeeding来初始化\n",
    "rnnmodel.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "rnnmodel.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T05:12:24.667958500Z",
     "start_time": "2024-01-04T03:28:51.292995800Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.6851194984844753 Train Acc: 0.5577714286259242\n",
      "Epoch: 0 Valid Loss: 0.6590333771387736 Valid Acc: 0.6042666666666666\n",
      "Epoch: 1 Train Loss: 0.6126890558310918 Train Acc: 0.6706857143265861\n",
      "Epoch: 1 Valid Loss: 0.5949398692131043 Valid Acc: 0.6788000000635783\n",
      "Epoch: 2 Train Loss: 0.6467643551690238 Train Acc: 0.6281142857551575\n",
      "Epoch: 2 Valid Loss: 0.5524442602793376 Valid Acc: 0.7370666666984558\n",
      "Epoch: 3 Train Loss: 0.5419273612976074 Train Acc: 0.7361714286395482\n",
      "Epoch: 3 Valid Loss: 0.40184845550855003 Valid Acc: 0.8366666667302449\n",
      "Epoch: 4 Train Loss: 0.34253443557875496 Train Acc: 0.8570857143265861\n",
      "Epoch: 4 Valid Loss: 0.29748059146404265 Valid Acc: 0.8821333333333333\n",
      "Epoch: 5 Train Loss: 0.25877757687228065 Train Acc: 0.9019428571428572\n",
      "Epoch: 5 Valid Loss: 0.2931904871781667 Valid Acc: 0.8857333333333334\n",
      "Epoch: 6 Train Loss: 0.19320900378567832 Train Acc: 0.9289142858232771\n",
      "Epoch: 6 Valid Loss: 0.28984688523610436 Valid Acc: 0.8806666666666667\n",
      "Epoch: 7 Train Loss: 0.1565488766874586 Train Acc: 0.9472000000817435\n",
      "Epoch: 7 Valid Loss: 0.35765631599624953 Valid Acc: 0.8845333333333333\n",
      "Epoch: 8 Train Loss: 0.15681237924269267 Train Acc: 0.9426285714558192\n",
      "Epoch: 8 Valid Loss: 0.3314703174849351 Valid Acc: 0.8814666666666666\n",
      "Epoch: 9 Train Loss: 0.10972013073989323 Train Acc: 0.9629714285986765\n",
      "Epoch: 9 Valid Loss: 0.3966884616658092 Valid Acc: 0.8810666666666667\n",
      "Epoch: 10 Train Loss: 0.08495131306137357 Train Acc: 0.9738857143129621\n",
      "Epoch: 10 Valid Loss: 0.3858635917534431 Valid Acc: 0.8822666666666666\n",
      "Epoch: 11 Train Loss: 0.07530260448179074 Train Acc: 0.9778285714285714\n",
      "Epoch: 11 Valid Loss: 0.3735387540998558 Valid Acc: 0.8802666666666666\n",
      "Epoch: 12 Train Loss: 0.06130596968957356 Train Acc: 0.9809142857142857\n",
      "Epoch: 12 Valid Loss: 0.48035219787011546 Valid Acc: 0.8757333333333334\n",
      "Epoch: 13 Train Loss: 0.05636152349923338 Train Acc: 0.9826857142857143\n",
      "Epoch: 13 Valid Loss: 0.4645497749780615 Valid Acc: 0.8738666666666667\n",
      "Epoch: 14 Train Loss: 0.050657630868894715 Train Acc: 0.9846285714285714\n",
      "Epoch: 14 Valid Loss: 0.48578939113033315 Valid Acc: 0.874\n",
      "Epoch: 15 Train Loss: 0.03767176600630794 Train Acc: 0.9893714285714286\n",
      "Epoch: 15 Valid Loss: 0.5407780545428396 Valid Acc: 0.8736\n",
      "Epoch: 16 Train Loss: 0.035585395135198324 Train Acc: 0.9900571428571429\n",
      "Epoch: 16 Valid Loss: 0.5473359313816453 Valid Acc: 0.8730666666666667\n",
      "Epoch: 17 Train Loss: 0.03282754706144333 Train Acc: 0.9901142857142857\n",
      "Epoch: 17 Valid Loss: 0.5854486148618162 Valid Acc: 0.8668\n",
      "Epoch: 18 Train Loss: 0.03142485954271895 Train Acc: 0.9913714285714286\n",
      "Epoch: 18 Valid Loss: 0.5787854401142647 Valid Acc: 0.8732\n",
      "Epoch: 19 Train Loss: 0.028185418097249098 Train Acc: 0.9926285714285714\n",
      "Epoch: 19 Valid Loss: 0.6132860740833606 Valid Acc: 0.8749333333333333\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "crit = nn.BCEWithLogitsLoss()#binary cross entropy\n",
    "optimizer = torch.optim.Adam(rnnmodel.parameters())\n",
    "rnnmodel = rnnmodel.to(device)\n",
    "N_EPOCHES = 20\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHES):\n",
    "    train_loss, train_acc = train(rnnmodel,train_iterator,optimizer,crit)\n",
    "    valid_loss, valid_acc = evaluate(rnnmodel,valid_iterator,crit)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(rnnmodel.state_dict(),\"rnn-best-model.pth\")\n",
    "    print(\"Epoch:\",epoch, \"Train Loss:\", train_loss, \"Train Acc:\", train_acc)\n",
    "    print(\"Epoch:\",epoch, \"Valid Loss:\", valid_loss, \"Valid Acc:\", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T05:14:26.985819200Z",
     "start_time": "2024-01-04T05:12:25.212989400Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnnmdoel模型在测试集上的表现为：\n",
      "测试损失Test Loss: 0.3320230337142944 测试精度Test Acc: 0.86844\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "rnnmodel.load_state_dict(torch.load(\"rnn-best-model.pth\"))\n",
    "test_loss2, test_acc2 = evaluate(rnnmodel,test_iterator,crit)\n",
    "print(\"rnnmdoel模型在测试集上的表现为：\")\n",
    "print(\"测试损失Test Loss:\", test_loss2, \"测试精度Test Acc:\", test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T05:14:56.063102200Z",
     "start_time": "2024-01-04T05:14:27.238856Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 利用模型来预测Sentiment\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device) # seq_len\n",
    "    tensor = tensor.unsqueeze(1) # seq_len * batch_size\n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T05:14:56.723947300Z",
     "start_time": "2024-01-04T05:14:56.063810800Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02515397220849991"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is wonderful!\",rnnmodel)#horrible,terrific,terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 拓展模型：CNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用卷积神经网络（CNN）实现文本情感分类任务的模型。\n",
    "\n",
    "CNNModel是一个继承自nn.Module的类，表示CNN模型。\n",
    "\n",
    "__init__函数初始化模型的各个组件。参数包括：vocab_size（词汇表大小）、embedding_size（嵌入层维度）、output_size（输出维度，即情感分类的类别数）、pad_idx（填充索引，用于将不同长度的输入序列填充为相同长度）、num_filters（卷积核的数量）、filter_sizes（卷积核的尺寸列表）和dropout（dropout的概率）。\n",
    "\n",
    "forward函数是模型的前向传播过程。输入text是一个整数张量，表示文本序列。首先将输入文本通过嵌入层进行词嵌入，得到形状为[sequence_len, batch_size, embedding_size]的张量。然后将其扩展一个维度，变为[batch_size, 1, sequence_len, embedding_size]的张量。接下来，通过卷积层和ReLU激活函数对嵌入的文本进行卷积操作，得到num_filters个特征图，形状为[batch_size, num_filters, sequence_len]。再通过最大池化层对每个特征图进行池化操作，得到形状为[batch_size, num_filters]的池化结果。最后，将所有池化结果拼接起来并经过dropout层和全连接层，得到最终的输出，形状为[batch_size, output_size]。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T08:05:29.137055700Z",
     "start_time": "2024-01-04T08:05:28.769048500Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#用CNN实现上述文本情感分类任务\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,output_size,pad_idx,num_filters,filter_sizes,dropout):\n",
    "        super(CNNModel,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,out_channels=num_filters,kernel_size=(fs,embedding_size)) for fs in filter_sizes])\n",
    "        self.linear = nn.Linear(num_filters*len(filter_sizes),output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,text):\n",
    "        embeded = self.embed(text) # [sequence_len, batch_size, embedding_size]\n",
    "        embeded = embeded.unsqueeze(1) #[batch_size, 1, sequence_len, embedding_size]\n",
    "        conved = [F.relu(conv(embeded)).squeeze(3) for conv in self.convs] #[batch_size, num_filters, sequence_len]\n",
    "        pooled = [F.max_pool1d(conv,conv.shape[2]).squeeze(2) for conv in conved] #[batch_size, num_filters]\n",
    "        cat = self.dropout(torch.cat(pooled,dim=1)) #[batch_size, num_filters*len(filter_sizes)]\n",
    "        return self.linear(cat)\n",
    "#模型的输入输出维度\n",
    "#输入：[sequence_len, batch_size]\n",
    "#输出：[batch_size, output_size]\n",
    "#model()传入的参数是[sequence_len,batch_size],为什么输出是[squence_len,output_size]?\n",
    "#因为在forward函数中，我们将输入的维度转换为[batch_size, 1, sequence_len, embedding_size]，然后经过卷积层和池化层，最后经过全连接层，得到输出\n",
    "#如何把结果改回来\n",
    "#在forward函数中，将输出的维度改为[batch_size, output_size]即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T08:05:29.144121100Z",
     "start_time": "2024-01-04T08:05:29.141063500Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "NUM_FILTERS = 5\n",
    "FILTER_SIZES = [3,4,5]\n",
    "DROPOUT = 0.5\n",
    "cnnmodel = CNNModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX,\n",
    "                     num_filters=NUM_FILTERS,\n",
    "                     filter_sizes=FILTER_SIZES,\n",
    "                     dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T08:05:29.145122100Z",
     "start_time": "2024-01-04T08:05:29.144121100Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 初始化Embedding矩阵\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "#pretrained_embedding.shape # [25002,100]\n",
    "cnnmodel.embed.weight.data.copy_(pretrained_embedding) #用事先训练好的embeeding来初始化\n",
    "cnnmodel.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "cnnmodel.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T08:05:29.210085400Z",
     "start_time": "2024-01-04T08:05:29.147049400Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.6193509819711958 Train Acc: 0.6563428572246006\n",
      "Epoch: 0 Valid Loss: 0.47043318322499594 Valid Acc: 0.8112000000317892\n",
      "Epoch: 1 Train Loss: 0.4606592519692012 Train Acc: 0.7941142858096532\n",
      "Epoch: 1 Valid Loss: 0.3961320514520009 Valid Acc: 0.8328000000317891\n",
      "Epoch: 2 Train Loss: 0.3949888648101262 Train Acc: 0.834228571496691\n",
      "Epoch: 2 Valid Loss: 0.35742811969121296 Valid Acc: 0.8441333333651225\n",
      "Epoch: 3 Train Loss: 0.3461907596213477 Train Acc: 0.862742857170105\n",
      "Epoch: 3 Valid Loss: 0.33911455365022025 Valid Acc: 0.8534666666984558\n",
      "Epoch: 4 Train Loss: 0.3026703739779336 Train Acc: 0.8824571429661342\n",
      "Epoch: 4 Valid Loss: 0.3311275888601939 Valid Acc: 0.8577333333651225\n",
      "Epoch: 5 Train Loss: 0.26505460402965547 Train Acc: 0.9029714286259243\n",
      "Epoch: 5 Valid Loss: 0.3298856813112895 Valid Acc: 0.8561333333651224\n",
      "Epoch: 6 Train Loss: 0.22963341992241995 Train Acc: 0.9176571429388863\n",
      "Epoch: 6 Valid Loss: 0.3373604797919591 Valid Acc: 0.8498666666666667\n",
      "Epoch: 7 Train Loss: 0.20497946143661228 Train Acc: 0.9261714285714285\n",
      "Epoch: 7 Valid Loss: 0.3415089457233747 Valid Acc: 0.8529333333651224\n",
      "Epoch: 8 Train Loss: 0.18088841659682137 Train Acc: 0.9375428571973529\n",
      "Epoch: 8 Valid Loss: 0.3472207624236743 Valid Acc: 0.8566666666984558\n",
      "Epoch: 9 Train Loss: 0.15615207063640868 Train Acc: 0.945542857170105\n",
      "Epoch: 9 Valid Loss: 0.36140820826292036 Valid Acc: 0.8532000000317892\n",
      "Epoch: 10 Train Loss: 0.14704230009147098 Train Acc: 0.9492000000272478\n",
      "Epoch: 10 Valid Loss: 0.37314804005622865 Valid Acc: 0.8542666666984559\n",
      "Epoch: 11 Train Loss: 0.1343105057307652 Train Acc: 0.9536571429388864\n",
      "Epoch: 11 Valid Loss: 0.382351157951355 Valid Acc: 0.8525333333651225\n",
      "Epoch: 12 Train Loss: 0.119021044921875 Train Acc: 0.9588000000272479\n",
      "Epoch: 12 Valid Loss: 0.40741620879968005 Valid Acc: 0.8489333333651224\n",
      "Epoch: 13 Train Loss: 0.10810860358817237 Train Acc: 0.9634857143129621\n",
      "Epoch: 13 Valid Loss: 0.4251493985056877 Valid Acc: 0.8482666666984559\n",
      "Epoch: 14 Train Loss: 0.10006490339551653 Train Acc: 0.9671428571428572\n",
      "Epoch: 14 Valid Loss: 0.4368037425716718 Valid Acc: 0.8485333333651225\n",
      "Epoch: 15 Train Loss: 0.09378958871023996 Train Acc: 0.9682285714830671\n",
      "Epoch: 15 Valid Loss: 0.45403679705460864 Valid Acc: 0.8486666666984558\n",
      "Epoch: 16 Train Loss: 0.08913514032874789 Train Acc: 0.9706285714285714\n",
      "Epoch: 16 Valid Loss: 0.4637063813706239 Valid Acc: 0.8418666666666667\n",
      "Epoch: 17 Train Loss: 0.07867777692249843 Train Acc: 0.9743428571428572\n",
      "Epoch: 17 Valid Loss: 0.49318983074824013 Valid Acc: 0.8477333333333333\n",
      "Epoch: 18 Train Loss: 0.07429163377795901 Train Acc: 0.9744000000272478\n",
      "Epoch: 18 Valid Loss: 0.5238012401660284 Valid Acc: 0.8454666666666667\n",
      "Epoch: 19 Train Loss: 0.07141570053441183 Train Acc: 0.9757142857415335\n",
      "Epoch: 19 Valid Loss: 0.5335220164060592 Valid Acc: 0.8429333333651224\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 训练模型\n",
    "def train(model, iterator, optimizer, crit):\n",
    "    \"\"\"\n",
    "    :param model: 训练模型\n",
    "    :param iterator: 数据\n",
    "    :param optimizer: 优化器\n",
    "    :param crit: 评估函数\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    total_len = 0.\n",
    "    model.train()    \n",
    "    for batch in iterator:\n",
    "        # forward pass\n",
    "        # print(\"batch.text.shape:\",batch.text.shape)\n",
    "        preds = model(batch.text.T).squeeze()\n",
    "        # print(\"preds.shape:\",preds.shape)\n",
    "        # print(\"batch.label.shape:\",batch.label.shape)\n",
    "        loss = crit(preds,batch.label)\n",
    "        acc = binary_accuracy(preds,batch.label)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() #梯度清零\n",
    "        loss.backward() # 反向传播，计算梯度\n",
    "        optimizer.step() #更新参数\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    return epoch_loss/total_len, epoch_acc/total_len\n",
    "def evaluate(model, iterator, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    total_len = 0.\n",
    "    model.eval()    \n",
    "    for batch in iterator:\n",
    "        # forward pass\n",
    "        preds = model(batch.text.T).squeeze()\n",
    "        loss = crit(preds,batch.label)\n",
    "        acc = binary_accuracy(preds,batch.label)\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    model.train() \n",
    "    return epoch_loss/total_len, epoch_acc/total_len\n",
    "crit = nn.BCEWithLogitsLoss()#binary cross entropy\n",
    "optimizer = torch.optim.Adam(cnnmodel.parameters())\n",
    "cnnmodel = cnnmodel.to(device)\n",
    "N_EPOCHES = 20\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHES):\n",
    "    train_loss, train_acc = train(cnnmodel,train_iterator,optimizer,crit)\n",
    "    valid_loss, valid_acc = evaluate(cnnmodel,valid_iterator,crit)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(cnnmodel.state_dict(),\"cnn-best-model.pth\")\n",
    "    print(\"Epoch:\",epoch, \"Train Loss:\", train_loss, \"Train Acc:\", train_acc)\n",
    "    print(\"Epoch:\",epoch, \"Valid Loss:\", valid_loss, \"Valid Acc:\", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.33639768340110776 Test Acc: 0.857080000038147\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试模型\n",
    "cnnmodel.load_state_dict(torch.load(\"cnn-best-model.pth\"))\n",
    "test_loss3, test_acc3 = evaluate(cnnmodel,test_iterator,crit)\n",
    "print(\"Test Loss:\", test_loss3, \"Test Acc:\", test_acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比三个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: WordAVGModel(\n",
      "  (embed): Embedding(25002, 100, padding_idx=1)\n",
      "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "Test Loss: 0.564186251986667 Test Acc: 0.724696291529614\n",
      "name: RNNModel(\n",
      "  (embed): Embedding(25002, 100, padding_idx=1)\n",
      "  (rnn): LSTM(100, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Test Loss: 0.564186251986667 Test Acc: 0.724696291529614\n",
      "name: CNNModel(\n",
      "  (embed): Embedding(25002, 100, padding_idx=1)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 5, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 5, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 5, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (linear): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Test Loss: 0.564186251986667 Test Acc: 0.724696291529614\n"
     ]
    }
   ],
   "source": [
    "#绘制三个模型测试精读的对比\n",
    "#对比三个模型在相同训练轮数时候的测试集上的表现对比\n",
    "wordmodel = WordAVGModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX)\n",
    "wordmodel.load_state_dict(torch.load(\"wordavg-best-model.pth\"))\n",
    "rnnmodel = RNNModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX,\n",
    "                     hidden_size=HIDDEN_SIZE,\n",
    "                     n_layers=N_LAYERS,\n",
    "                     bidirectional=BIDIRECTIONAL,\n",
    "                     dropout=DROPOUT)\n",
    "rnnmodel.load_state_dict(torch.load(\"rnn-best-model.pth\"))\n",
    "cnnmodel = CNNModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_size=EMBEDDING_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX,\n",
    "                     num_filters=NUM_FILTERS,\n",
    "                     filter_sizes=FILTER_SIZES,\n",
    "                     dropout=DROPOUT)\n",
    "cnnmodel.load_state_dict(torch.load(\"cnn-best-model.pth\"))\n",
    "models = [wordmodel,rnnmodel,cnnmodel]\n",
    "crit = nn.BCEWithLogitsLoss()#binary cross entropy\n",
    "# for i in models:\n",
    "#     # test_loss, test_acc = evaluate(i,test_iterator,crit)\n",
    "#     print(\"name:\",i)\n",
    "#     print(\"Test Loss:\", test_loss, \"Test Acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n",
      "findfont: Font family 'SimHei' not found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 已知三个值绘制一个柱状图\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "plt.figure(figsize=(6,6))\n",
    "x=np.arange(3)\n",
    "y=[test_acc1,test_acc2,test_acc3]\n",
    "plt.bar(x,y,width=0.5,align='center',color='c',alpha=0.5)\n",
    "plt.xticks(x,['Word Averaging','RNN','CNN'])\n",
    "plt.xlabel('模型类别')\n",
    "plt.ylabel('测试精度')\n",
    "plt.title('三种模型测试精度')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里的图是因为不支持中文字体出现了方框"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关扩展实验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调试代码时候撰写的其他rnn和cnn代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.667 | Train Acc: 58.77%\n",
      "\tValid Loss: 0.614 | Valid Acc: 67.49%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.596 | Train Acc: 68.07%\n",
      "\tValid Loss: 0.592 | Valid Acc: 68.68%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.556 | Train Acc: 72.14%\n",
      "\tValid Loss: 0.583 | Valid Acc: 69.45%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.453 | Train Acc: 79.49%\n",
      "\tValid Loss: 0.565 | Valid Acc: 72.99%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.409 | Train Acc: 82.39%\n",
      "\tValid Loss: 0.405 | Valid Acc: 82.29%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.539 | Train Acc: 71.18%\n",
      "\tValid Loss: 0.664 | Valid Acc: 57.92%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.616 | Train Acc: 66.93%\n",
      "\tValid Loss: 0.638 | Valid Acc: 66.77%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.515 | Train Acc: 76.94%\n",
      "\tValid Loss: 0.520 | Valid Acc: 72.41%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.362 | Train Acc: 86.62%\n",
      "\tValid Loss: 0.361 | Valid Acc: 85.60%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.259 | Train Acc: 91.03%\n",
      "\tValid Loss: 0.342 | Valid Acc: 86.47%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.216 | Train Acc: 92.99%\n",
      "\tValid Loss: 0.334 | Valid Acc: 86.52%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.166 | Train Acc: 94.90%\n",
      "\tValid Loss: 0.353 | Valid Acc: 87.32%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.124 | Train Acc: 96.57%\n",
      "\tValid Loss: 0.565 | Valid Acc: 84.32%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.108 | Train Acc: 97.14%\n",
      "\tValid Loss: 0.402 | Valid Acc: 87.45%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.080 | Train Acc: 98.14%\n",
      "\tValid Loss: 0.462 | Valid Acc: 87.51%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.072 | Train Acc: 98.40%\n",
      "\tValid Loss: 0.502 | Valid Acc: 86.97%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.67%\n",
      "\tValid Loss: 0.512 | Valid Acc: 87.33%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.83%\n",
      "\tValid Loss: 0.509 | Valid Acc: 87.05%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.93%\n",
      "\tValid Loss: 0.545 | Valid Acc: 86.80%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.039 | Train Acc: 99.25%\n",
      "\tValid Loss: 0.521 | Valid Acc: 86.81%\n",
      "Test Loss: 0.388 | Test Acc: 84.31%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "def train(model, iterator, optimizer, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    total_len = 0.\n",
    "    model.train()    \n",
    "    for batch in iterator:\n",
    "        # forward pass\n",
    "        # print(\"batch.text.shape:\",batch.text.shape)\n",
    "        preds = model(batch.text).squeeze()\n",
    "        # print(\"preds.shape:\",preds.shape)\n",
    "        # print(\"batch.label.shape:\",batch.label.shape)\n",
    "        loss = crit(preds,batch.label)\n",
    "        acc = binary_accuracy(preds,batch.label)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() #梯度清零\n",
    "        loss.backward() # 反向传播，计算梯度\n",
    "        optimizer.step() #更新参数\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    return epoch_loss/total_len, epoch_acc/total_len\n",
    "def evaluate(model, iterator, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    total_len = 0.\n",
    "    model.eval()    \n",
    "    for batch in iterator:\n",
    "        # forward pass\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = crit(preds,batch.label)\n",
    "        acc = binary_accuracy(preds,batch.label)\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    model.train() \n",
    "    return epoch_loss/total_len, epoch_acc/total_len\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 定义模型参数\n",
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "# 创建模型实例\n",
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# 开始训练\n",
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "# 加载最佳模型并评估测试数据\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.715 | Train Acc: 51.79%\n",
      "\t Val. Loss: 0.671 |  Val. Acc: 63.68%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.675 | Train Acc: 56.67%\n",
      "\t Val. Loss: 0.647 |  Val. Acc: 66.90%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.655 | Train Acc: 59.65%\n",
      "\t Val. Loss: 0.620 |  Val. Acc: 69.74%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.630 | Train Acc: 63.89%\n",
      "\t Val. Loss: 0.594 |  Val. Acc: 70.93%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.607 | Train Acc: 66.90%\n",
      "\t Val. Loss: 0.567 |  Val. Acc: 72.37%\n",
      "Test Loss: 0.564 | Test Acc: 72.47%\n"
     ]
    }
   ],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.unsqueeze(1)  # 在第二个维度上添加一个维度，用于卷积操作\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        output = self.fc(cat)\n",
    "        return output\n",
    "\n",
    "# 定义模型参数\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 50\n",
    "n_filters = 3\n",
    "filter_sizes = [3, 4, 5]\n",
    "output_dim = 1\n",
    "dropout = 0.5\n",
    "\n",
    "# 创建模型实例\n",
    "model = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout)\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "model = model.to(device)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text = batch.text.to(device)\n",
    "        labels = batch.label.to(device)\n",
    "        predictions = model(text.T).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# 测试模型\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text.to(device)\n",
    "            labels = batch.label.to(device)\n",
    "            predictions = model(text.T).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# 定义计算准确率的函数\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# 开始训练\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "# 进行测试\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是作业中写过的一些代码，也是rnn和cnn，因为效果不同，采取了上面的版本而没有采取这些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "实验中我们利用三个模型进行了文本的情感分类实验，分别是WordAVGModel，RNNModel和CNNModel。\n",
    "我们尽量保持在相同训练情况下对比不同模型的实验效果，我们可以看到：\n",
    "\n",
    "但是上述对比并不公平，因为三个模型的参数量不同，所以我们可以对比不同模型的参数量，然后对比不同模型的测试集上的表现。\n",
    "模型的结构业务相同很难进行横向对比，比如CNN模型中的卷积核的个数，卷积核的大小，RNN模型中的隐藏层的大小，LSTM的层数等等。只能大概的观看在相同训练轮数下的测试集上的表现。整体效果上CNN要优于其他两个模型，但是CNN的参数量也是最大的。训练起来十分吃力。WordAVGModel的参数量最少，训练起来也是最快的，。RNN模型的参数量介于两者之间，训练起来也比较快。而三个模型的具体效果，比较随机，在尝试作业的时候多次训练，三者之间的相对关系一直有所变化，故没有固定的规律。很多时候模型的精读取决于参数和模型训练中的一些小技巧。上述的精度对比图说明不了什么结论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
